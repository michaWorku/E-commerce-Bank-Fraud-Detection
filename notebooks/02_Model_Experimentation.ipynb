{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_Model_Experimentation.ipynb\n",
    "\n",
    "This notebook covers the model building and training phase for the fraud detection project, including:\n",
    "1.  **Data Preparation:** Feature/target separation and stratified train-test splitting.\n",
    "2.  **Model Selection:** Training Logistic Regression (baseline) and XGBoost (ensemble) models.\n",
    "3.  **Model Evaluation:** Assessing performance using metrics suitable for imbalanced data (AUC-PR, F1-Score, Confusion Matrix).\n",
    "4.  **Model Interpretation:** Basic insights using SHAP and LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if you haven't already in your Jupyter environment.\n",
    "# You can uncomment and run these lines if needed, or install via requirements.txt:\n",
    "# !pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn xgboost shap lime\n",
    "# If using GPU and cuDF is available, ensure it's installed in your environment:\n",
    "# !pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com # Example for CUDA 12\n",
    "\n",
    "# Core Libraries for Data Manipulation and Numerical Operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn for Data Splitting and Imbalance Handling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE # For handling class imbalance\n",
    "\n",
    "# Scikit-learn base classes and components (needed for custom transformers and pipelines)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Attempt to import cudf for GPU acceleration in the notebook scope\n",
    "try:\n",
    "    import cudf\n",
    "    _CUDF_AVAILABLE_NOTEBOOK = True\n",
    "    print(\"cuDF is available. GPU acceleration will be attempted where supported.\")\n",
    "except ImportError:\n",
    "    _CUDF_AVAILABLE_NOTEBOOK = False\n",
    "    print(\"cuDF not available. All operations will use pandas (CPU).\")\n",
    "\n",
    "# Add the project root directory to the system path.\n",
    "# This allows Python to correctly locate and import our custom modules\n",
    "# (e.g., from `src.data_processing.loader`) regardless of where the notebook is run from.\n",
    "current_notebook_path = Path.cwd()\n",
    "if current_notebook_path.name == 'notebooks':\n",
    "    project_root = current_notebook_path.parent\n",
    "else:\n",
    "    project_root = current_notebook_path\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import Custom Modular Functions and Classes\n",
    "from src.data_processing.loader import load_data\n",
    "# Note: FraudDataProcessor and CreditCardDataProcessor are used in run_data_pipeline,\n",
    "# here we directly load the processed data.\n",
    "# from src.data_processing.preprocessor import FraudDataProcessor, CreditCardDataProcessor\n",
    "# from src.utils.helpers import merge_ip_to_country\n",
    "\n",
    "# Import Model Strategies, Trainer, Evaluator, and Interpreter\n",
    "from src.models.logistic_regression_strategy import LogisticRegressionStrategy\n",
    "from src.models.xgboost_strategy import XGBoostStrategy # Choosing XGBoost as the ensemble model\n",
    "from src.models.random_forest_strategy import RandomForestStrategy # Also available if you prefer\n",
    "from src.models.model_trainer import ModelTrainer\n",
    "from src.models.model_evaluator import evaluate_classification_model\n",
    "from src.models.model_interpreter import ModelInterpreter\n",
    "\n",
    "# Import constants from run_data_pipeline.py for use throughout the notebook\n",
    "from scripts.run_data_pipeline import (\n",
    "    PROCESSED_DATA_DIR,\n",
    "    FRAUD_TARGET_COL,\n",
    "    CREDITCARD_TARGET_COL,\n",
    "    # CREDITCARD_NUMERICAL_FEATURES, # Not directly needed here as we load processed data\n",
    "    # FRAUD_NUMERICAL_FEATURES, # Not directly needed here as we load processed data\n",
    "    # FRAUD_CATEGORICAL_FEATURES # Not directly needed here as we load processed data\n",
    ")\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Preprocessed Data\n",
    "Load the preprocessed datasets (`fraud_processed.csv` and `creditcard_processed.csv`) generated by the data pipeline. These DataFrames should contain clean, transformed features ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for preprocessed data\n",
    "FRAUD_PROCESSED_PATH = PROCESSED_DATA_DIR / \"fraud_processed.csv\"\n",
    "CREDITCARD_PROCESSED_PATH = PROCESSED_DATA_DIR / \"creditcard_processed.csv\"\n",
    "\n",
    "# Load processed data. Ensure it's in pandas DataFrame format for scikit-learn compatibility.\n",
    "# The `load_data` function should handle cuDF to pandas conversion if use_gpu is True.\n",
    "print(\"--- Loading Preprocessed Datasets ---\")\n",
    "\n",
    "fraud_processed_df = load_data(FRAUD_PROCESSED_PATH, use_gpu=False) # Force pandas for consistency with sklearn\n",
    "creditcard_processed_df = load_data(CREDITCARD_PROCESSED_PATH, use_gpu=False) # Force pandas for consistency with sklearn\n",
    "\n",
    "if fraud_processed_df.empty:\n",
    "    print(f\"Error: E-commerce Fraud Data not loaded from {FRAUD_PROCESSED_PATH}. Please ensure `01_EDA.ipynb` or `run_data_pipeline.py` has been run.\")\n",
    "if creditcard_processed_df.empty:\n",
    "    print(f\"Error: Bank Credit Card Fraud Data not loaded from {CREDITCARD_PROCESSED_PATH}. Please ensure `01_EDA.ipynb` or `run_data_pipeline.py` has been run.\")\n",
    "\n",
    "print(\"\\nPreprocessed Data Info (Fraud):\")\n",
    "fraud_processed_df.info()\n",
    "print(\"\\nPreprocessed Data Info (Credit Card):\")\n",
    "creditcard_processed_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preparation - E-commerce Fraud Data\n",
    "Separate features (X) and target (y), then perform a stratified train-test split. Class imbalance handling (SMOTE) is applied only to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Data Preparation: E-commerce Fraud Data ---\")\n",
    "\n",
    "if not fraud_processed_df.empty:\n",
    "    # Separate features (X) and target (y)\n",
    "    # Identify feature columns: all columns except the target\n",
    "    fraud_feature_cols = [col for col in fraud_processed_df.columns if col != FRAUD_TARGET_COL]\n",
    "    \n",
    "    X_fraud = fraud_processed_df[fraud_feature_cols]\n",
    "    y_fraud = fraud_processed_df[FRAUD_TARGET_COL]\n",
    "\n",
    "    # Ensure target is integer type for stratification\n",
    "    y_fraud = y_fraud.astype(int)\n",
    "\n",
    "    print(f\"Original E-commerce Fraud Data shape: {X_fraud.shape}\")\n",
    "    print(f\"Original E-commerce Fraud Target distribution:\\n{y_fraud.value_counts(normalize=True)}\")\n",
    "\n",
    "    # Train-Test Split with stratification\n",
    "    X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(\n",
    "        X_fraud, y_fraud, test_size=0.2, random_state=RANDOM_STATE, stratify=y_fraud\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFraud Train set shape: {X_train_fraud.shape}\")\n",
    "    print(f\"Fraud Test set shape: {X_test_fraud.shape}\")\n",
    "    print(f\"Fraud Train target distribution:\\n{y_train_fraud.value_counts(normalize=True)}\")\n",
    "    print(f\"Fraud Test target distribution:\\n{y_test_fraud.value_counts(normalize=True)}\")\n",
    "\n",
    "    # Apply SMOTE to the training data only\n",
    "    print(\"\\nApplying SMOTE to E-commerce Fraud training data...\")\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_fraud_resampled, y_train_fraud_resampled = smote.fit_resample(X_train_fraud, y_train_fraud)\n",
    "\n",
    "    print(f\"Fraud Train set shape after SMOTE: {X_train_fraud_resampled.shape}\")\n",
    "    print(f\"Fraud Train target distribution after SMOTE:\\n{y_train_fraud_resampled.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"E-commerce Fraud Data is empty. Skipping data preparation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training & Evaluation - E-commerce Fraud Data (Logistic Regression)\n",
    "Train and evaluate a Logistic Regression model as a baseline for the E-commerce Fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Training & Evaluation: E-commerce Fraud Data (Logistic Regression) ---\")\n",
    "\n",
    "if not fraud_processed_df.empty:\n",
    "    # Initialize Logistic Regression model strategy\n",
    "    lr_strategy_fraud = LogisticRegressionStrategy(random_state=RANDOM_STATE, solver='liblinear', max_iter=1000)\n",
    "    \n",
    "    # Initialize ModelTrainer\n",
    "    trainer_fraud_lr = ModelTrainer(lr_strategy_fraud)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer_fraud_lr.train_model(X_train_fraud_resampled, y_train_fraud_resampled)\n",
    "    \n",
    "    # Make predictions (probabilities for positive class)\n",
    "    y_pred_proba_fraud_lr = trainer_fraud_lr.predict_model(X_test_fraud)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\n--- Evaluation Metrics (Logistic Regression, E-commerce Fraud) ---\")\n",
    "    lr_fraud_metrics = evaluate_classification_model(y_test_fraud.values, y_pred_proba_fraud_lr)\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    y_pred_binary_fraud_lr = (y_pred_proba_fraud_lr >= 0.5).astype(int)\n",
    "    cm_fraud_lr = confusion_matrix(y_test_fraud, y_pred_binary_fraud_lr)\n",
    "    disp_fraud_lr = ConfusionMatrixDisplay(confusion_matrix=cm_fraud_lr, display_labels=[0, 1])\n",
    "    disp_fraud_lr.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix: Logistic Regression (E-commerce Fraud)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"E-commerce Fraud Data is empty. Skipping Logistic Regression training and evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training & Evaluation - E-commerce Fraud Data (XGBoost Classifier)\n",
    "Train and evaluate an XGBoost Classifier for the E-commerce Fraud dataset, leveraging its ensemble power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Training & Evaluation: E-commerce Fraud Data (XGBoost Classifier) ---\")\n",
    "\n",
    "if not fraud_processed_df.empty:\n",
    "    # Initialize XGBoost model strategy\n",
    "    # Use a few common hyperparameters for a start.\n",
    "    # With SMOTE, `scale_pos_weight` might not be strictly necessary, but can be fine-tuned.\n",
    "    xgb_strategy_fraud = XGBoostStrategy(\n",
    "        model_type='classifier',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=200,      # Number of boosting rounds\n",
    "        learning_rate=0.1,     # Step size shrinkage\n",
    "        max_depth=5,           # Maximum depth of a tree\n",
    "        subsample=0.8,         # Subsample ratio of the training instance\n",
    "        colsample_bytree=0.8,  # Subsample ratio of columns when constructing each tree\n",
    "        use_label_encoder=False, # Suppress warning\n",
    "        eval_metric='logloss' # Evaluation metric for validation data\n",
    "    )\n",
    "    \n",
    "    # Initialize ModelTrainer\n",
    "    trainer_fraud_xgb = ModelTrainer(xgb_strategy_fraud)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer_fraud_xgb.train_model(X_train_fraud_resampled, y_train_fraud_resampled)\n",
    "    \n",
    "    # Make predictions (probabilities for positive class)\n",
    "    y_pred_proba_fraud_xgb = trainer_fraud_xgb.predict_model(X_test_fraud)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\n--- Evaluation Metrics (XGBoost Classifier, E-commerce Fraud) ---\")\n",
    "    xgb_fraud_metrics = evaluate_classification_model(y_test_fraud.values, y_pred_proba_fraud_xgb)\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    y_pred_binary_fraud_xgb = (y_pred_proba_fraud_xgb >= 0.5).astype(int)\n",
    "    cm_fraud_xgb = confusion_matrix(y_test_fraud, y_pred_binary_fraud_xgb)\n",
    "    disp_fraud_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_fraud_xgb, display_labels=[0, 1])\n",
    "    disp_fraud_xgb.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix: XGBoost Classifier (E-commerce Fraud)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"E-commerce Fraud Data is empty. Skipping XGBoost training and evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Interpretation - E-commerce Fraud Data (XGBoost)\n",
    "Interpret the XGBoost model's predictions using SHAP for global feature importance and LIME for local instance explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Interpretation: E-commerce Fraud Data (XGBoost) ---\")\n",
    "\n",
    "if not fraud_processed_df.empty and trainer_fraud_xgb.trained_model is not None:\n",
    "    # Get the feature names from the processed training data\n",
    "    feature_names_fraud = X_train_fraud_resampled.columns.tolist()\n",
    "    \n",
    "    # Initialize ModelInterpreter\n",
    "    interpreter_fraud_xgb = ModelInterpreter(\n",
    "        model=trainer_fraud_xgb.get_current_model_object(),\n",
    "        feature_names=feature_names_fraud,\n",
    "        model_type='classification',\n",
    "        class_names=['Non-Fraud', 'Fraud'],\n",
    "        training_data_for_lime=X_train_fraud_resampled # LIME needs original training data for background\n",
    "    )\n",
    "\n",
    "    # Global Feature Importance (SHAP Summary Plot)\n",
    "    interpreter_fraud_xgb.explain_model_shap(X_test_fraud)\n",
    "    interpreter_fraud_xgb.plot_shap_summary(X_test_fraud)\n",
    "\n",
    "    # Local Interpretation (LIME) - Explain a sample fraudulent transaction\n",
    "    # Find an actual fraudulent transaction in the test set\n",
    "    fraud_indices = y_test_fraud[y_test_fraud == 1].index\n",
    "    if not fraud_indices.empty:\n",
    "        sample_fraud_instance_idx = fraud_indices[0] # Take the first one\n",
    "        sample_fraud_instance = X_test_fraud.loc[sample_fraud_instance_idx]\n",
    "        print(f\"\\nExplaining a sample fraudulent instance (Index: {sample_fraud_instance_idx})...\")\n",
    "        interpreter_fraud_xgb.explain_instance_lime(sample_fraud_instance)\n",
    "    else:\n",
    "        print(\"No fraudulent instances found in the test set for LIME explanation.\")\n",
    "\n",
    "    # Local Interpretation (LIME) - Explain a sample non-fraudulent transaction\n",
    "    non_fraud_indices = y_test_fraud[y_test_fraud == 0].index\n",
    "    if not non_fraud_indices.empty:\n",
    "        sample_non_fraud_instance_idx = non_fraud_indices[0] # Take the first one\n",
    "        sample_non_fraud_instance = X_test_fraud.loc[sample_non_fraud_instance_idx]\n",
    "        print(f\"\\nExplaining a sample non-fraudulent instance (Index: {sample_non_fraud_instance_idx})...\")\n",
    "        interpreter_fraud_xgb.explain_instance_lime(sample_non_fraud_instance)\n",
    "    else:\n",
    "        print(\"No non-fraudulent instances found in the test set for LIME explanation.\")\n",
    "\n",
    "else:\n",
    "    print(\"E-commerce Fraud Data is empty or XGBoost model not trained. Skipping interpretation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Data Preparation - Credit Card Fraud Data\n",
    "Separate features (X) and target (y), then perform a stratified train-test split. Class imbalance handling (SMOTE) is applied only to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Data Preparation: Credit Card Fraud Data ---\")\n",
    "\n",
    "if not creditcard_processed_df.empty:\n",
    "    # Separate features (X) and target (y)\n",
    "    creditcard_feature_cols = [col for col in creditcard_processed_df.columns if col != CREDITCARD_TARGET_COL]\n",
    "    \n",
    "    X_creditcard = creditcard_processed_df[creditcard_feature_cols]\n",
    "    y_creditcard = creditcard_processed_df[CREDITCARD_TARGET_COL]\n",
    "\n",
    "    # Ensure target is integer type for stratification\n",
    "    y_creditcard = y_creditcard.astype(int)\n",
    "\n",
    "    print(f\"Original Credit Card Fraud Data shape: {X_creditcard.shape}\")\n",
    "    print(f\"Original Credit Card Fraud Target distribution:\\n{y_creditcard.value_counts(normalize=True)}\")\n",
    "\n",
    "    # Train-Test Split with stratification\n",
    "    X_train_creditcard, X_test_creditcard, y_train_creditcard, y_test_creditcard = train_test_split(\n",
    "        X_creditcard, y_creditcard, test_size=0.2, random_state=RANDOM_STATE, stratify=y_creditcard\n",
    "    )\n",
    "\n",
    "    print(f\"\\nCredit Card Train set shape: {X_train_creditcard.shape}\")\n",
    "    print(f\"Credit Card Test set shape: {X_test_creditcard.shape}\")\n",
    "    print(f\"Credit Card Train target distribution:\\n{y_train_creditcard.value_counts(normalize=True)}\")\n",
    "    print(f\"Credit Card Test target distribution:\\n{y_test_creditcard.value_counts(normalize=True)}\")\n",
    "\n",
    "    # Apply SMOTE to the training data only\n",
    "    print(\"\\nApplying SMOTE to Credit Card Fraud training data...\")\n",
    "    smote = SMOTE(random_state=RANDOM_STATE)\n",
    "    X_train_creditcard_resampled, y_train_creditcard_resampled = smote.fit_resample(X_train_creditcard, y_train_creditcard)\n",
    "\n",
    "    print(f\"Credit Card Train set shape after SMOTE: {X_train_creditcard_resampled.shape}\")\n",
    "    print(f\"Credit Card Train target distribution after SMOTE:\\n{y_train_creditcard_resampled.value_counts(normalize=True)}\")\n",
    "else:\n",
    "    print(\"Credit Card Fraud Data is empty. Skipping data preparation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Training & Evaluation - Credit Card Fraud Data (Logistic Regression)\n",
    "Train and evaluate a Logistic Regression model as a baseline for the Credit Card Fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Training & Evaluation: Credit Card Fraud Data (Logistic Regression) ---\")\n",
    "\n",
    "if not creditcard_processed_df.empty:\n",
    "    # Initialize Logistic Regression model strategy\n",
    "    lr_strategy_creditcard = LogisticRegressionStrategy(random_state=RANDOM_STATE, solver='liblinear', max_iter=1000)\n",
    "    \n",
    "    # Initialize ModelTrainer\n",
    "    trainer_creditcard_lr = ModelTrainer(lr_strategy_creditcard)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer_creditcard_lr.train_model(X_train_creditcard_resampled, y_train_creditcard_resampled)\n",
    "    \n",
    "    # Make predictions (probabilities for positive class)\n",
    "    y_pred_proba_creditcard_lr = trainer_creditcard_lr.predict_model(X_test_creditcard)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\n--- Evaluation Metrics (Logistic Regression, Credit Card Fraud) ---\")\n",
    "    lr_creditcard_metrics = evaluate_classification_model(y_test_creditcard.values, y_pred_proba_creditcard_lr)\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    y_pred_binary_creditcard_lr = (y_pred_proba_creditcard_lr >= 0.5).astype(int)\n",
    "    cm_creditcard_lr = confusion_matrix(y_test_creditcard, y_pred_binary_creditcard_lr)\n",
    "    disp_creditcard_lr = ConfusionMatrixDisplay(confusion_matrix=cm_creditcard_lr, display_labels=[0, 1])\n",
    "    disp_creditcard_lr.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix: Logistic Regression (Credit Card Fraud)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Credit Card Fraud Data is empty. Skipping Logistic Regression training and evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Model Training & Evaluation - Credit Card Fraud Data (XGBoost Classifier)\n",
    "Train and evaluate an XGBoost Classifier for the Credit Card Fraud dataset, leveraging its ensemble power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Training & Evaluation: Credit Card Fraud Data (XGBoost Classifier) ---\")\n",
    "\n",
    "if not creditcard_processed_df.empty:\n",
    "    # Initialize XGBoost model strategy\n",
    "    xgb_strategy_creditcard = XGBoostStrategy(\n",
    "        model_type='classifier',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    \n",
    "    # Initialize ModelTrainer\n",
    "    trainer_creditcard_xgb = ModelTrainer(xgb_strategy_creditcard)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer_creditcard_xgb.train_model(X_train_creditcard_resampled, y_train_creditcard_resampled)\n",
    "    \n",
    "    # Make predictions (probabilities for positive class)\n",
    "    y_pred_proba_creditcard_xgb = trainer_creditcard_xgb.predict_model(X_test_creditcard)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\n--- Evaluation Metrics (XGBoost Classifier, Credit Card Fraud) ---\")\n",
    "    xgb_creditcard_metrics = evaluate_classification_model(y_test_creditcard.values, y_pred_proba_creditcard_xgb)\n",
    "\n",
    "    # Display Confusion Matrix\n",
    "    y_pred_binary_creditcard_xgb = (y_pred_proba_creditcard_xgb >= 0.5).astype(int)\n",
    "    cm_creditcard_xgb = confusion_matrix(y_test_creditcard, y_pred_binary_creditcard_xgb)\n",
    "    disp_creditcard_xgb = ConfusionMatrixDisplay(confusion_matrix=cm_creditcard_xgb, display_labels=[0, 1])\n",
    "    disp_creditcard_xgb.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix: XGBoost Classifier (Credit Card Fraud)')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Credit Card Fraud Data is empty. Skipping XGBoost training and evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Model Interpretation - Credit Card Fraud Data (XGBoost)\n",
    "Interpret the XGBoost model's predictions using SHAP for global feature importance and LIME for local instance explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Model Interpretation: Credit Card Fraud Data (XGBoost) ---\")\n",
    "\n",
    "if not creditcard_processed_df.empty and trainer_creditcard_xgb.trained_model is not None:\n",
    "    # Get the feature names from the processed training data\n",
    "    feature_names_creditcard = X_train_creditcard_resampled.columns.tolist()\n",
    "    \n",
    "    # Initialize ModelInterpreter\n",
    "    interpreter_creditcard_xgb = ModelInterpreter(\n",
    "        model=trainer_creditcard_xgb.get_current_model_object(),\n",
    "        feature_names=feature_names_creditcard,\n",
    "        model_type='classification',\n",
    "        class_names=['Non-Fraud', 'Fraud'],\n",
    "        training_data_for_lime=X_train_creditcard_resampled # LIME needs original training data for background\n",
    "    )\n",
    "\n",
    "    # Global Feature Importance (SHAP Summary Plot)\n",
    "    interpreter_creditcard_xgb.explain_model_shap(X_test_creditcard)\n",
    "    interpreter_creditcard_xgb.plot_shap_summary(X_test_creditcard)\n",
    "\n",
    "    # Local Interpretation (LIME) - Explain a sample fraudulent transaction\n",
    "    fraud_indices_cc = y_test_creditcard[y_test_creditcard == 1].index\n",
    "    if not fraud_indices_cc.empty:\n",
    "        sample_fraud_instance_idx_cc = fraud_indices_cc[0]\n",
    "        sample_fraud_instance_cc = X_test_creditcard.loc[sample_fraud_instance_idx_cc]\n",
    "        print(f\"\\nExplaining a sample fraudulent instance (Index: {sample_fraud_instance_idx_cc})...\")\n",
    "        interpreter_creditcard_xgb.explain_instance_lime(sample_fraud_instance_cc)\n",
    "    else:\n",
    "        print(\"No fraudulent instances found in the Credit Card test set for LIME explanation.\")\n",
    "\n",
    "    # Local Interpretation (LIME) - Explain a sample non-fraudulent transaction\n",
    "    non_fraud_indices_cc = y_test_creditcard[y_test_creditcard == 0].index\n",
    "    if not non_fraud_indices_cc.empty:\n",
    "        sample_non_fraud_instance_idx_cc = non_fraud_indices_cc[0]\n",
    "        sample_non_fraud_instance_cc = X_test_creditcard.loc[sample_non_fraud_instance_idx_cc]\n",
    "        print(f\"\\nExplaining a sample non-fraudulent instance (Index: {sample_non_fraud_instance_idx_cc})...\")\n",
    "        interpreter_creditcard_xgb.explain_instance_lime(sample_non_fraud_instance_cc)\n",
    "    else:\n",
    "        print(\"No non-fraudulent instances found in the Credit Card test set for LIME explanation.\")\n",
    "\n",
    "else:\n",
    "    print(\"Credit Card Fraud Data is empty or XGBoost model not trained. Skipping interpretation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Model Justification and Conclusion\n",
    "\n",
    "**E-commerce Fraud Data:**\n",
    "- **Logistic Regression Metrics:**\n",
    "  - Accuracy: {{ lr_fraud_metrics.get('Accuracy', 'N/A') }}\n",
    "  - Precision: {{ lr_fraud_metrics.get('Precision', 'N/A') }}\n",
    "  - Recall: {{ lr_fraud_metrics.get('Recall', 'N/A') }}\n",
    "  - F1-score: {{ lr_fraud_metrics.get('F1-score', 'N/A') }}\n",
    "  - ROC-AUC: {{ lr_fraud_metrics.get('ROC-AUC', 'N/A') }}\n",
    "\n",
    "- **XGBoost Classifier Metrics:**\n",
    "  - Accuracy: {{ xgb_fraud_metrics.get('Accuracy', 'N/A') }}\n",
    "  - Precision: {{ xgb_fraud_metrics.get('Precision', 'N/A') }}\n",
    "  - Recall: {{ xgb_fraud_metrics.get('Recall', 'N/A') }}\n",
    "  - F1-score: {{ xgb_fraud_metrics.get('F1-score', 'N/A') }}\n",
    "  - ROC-AUC: {{ xgb_fraud_metrics.get('ROC-AUC', 'N/A') }}\n",
    "\n",
    "**Credit Card Fraud Data:**\n",
    "- **Logistic Regression Metrics:**\n",
    "  - Accuracy: {{ lr_creditcard_metrics.get('Accuracy', 'N/A') }}\n",
    "  - Precision: {{ lr_creditcard_metrics.get('Precision', 'N/A') }}\n",
    "  - Recall: {{ lr_creditcard_metrics.get('Recall', 'N/A') }}\n",
    "  - F1-score: {{ lr_creditcard_metrics.get('F1-score', 'N/A') }}\n",
    "  - ROC-AUC: {{ lr_creditcard_metrics.get('ROC-AUC', 'N/A') }}\n",
    "\n",
    "- **XGBoost Classifier Metrics:**\n",
    "  - Accuracy: {{ xgb_creditcard_metrics.get('Accuracy', 'N/A') }}\n",
    "  - Precision: {{ xgb_creditcard_metrics.get('Precision', 'N/A') }}\n",
    "  - Recall: {{ xgb_creditcard_metrics.get('Recall', 'N/A') }}\n",
    "  - F1-score: {{ xgb_creditcard_metrics.get('F1-score', 'N/A') }}\n",
    "  - ROC-AUC: {{ xgb_creditcard_metrics.get('ROC-AUC', 'N/A') }}\n",
    "\n",
    "---\n",
    "\n",
    "**Justification of the Best Model:**\n",
    "\n",
    "For both datasets, the **XGBoost Classifier** is generally expected to outperform Logistic Regression, especially on metrics like F1-score and ROC-AUC, which are crucial for imbalanced datasets.\n",
    "\n",
    "**Why XGBoost is preferred for Fraud Detection:**\n",
    "1.  **Handles Imbalance (with SMOTE):** Even with SMOTE, tree-based models like XGBoost are robust to imbalanced classes and can learn complex decision boundaries.\n",
    "2.  **Feature Interactions:** XGBoost can automatically learn non-linear relationships and interactions between features, which are common in fraud patterns. Logistic Regression assumes linearity.\n",
    "3.  **Performance on Tabular Data:** Gradient Boosting models consistently achieve state-of-the-art performance on tabular datasets.\n",
    "4.  **Interpretability (with SHAP/LIME):** While more complex than Logistic Regression, tools like SHAP and LIME allow us to interpret its predictions, identifying key features contributing to fraud scores, which is vital for understanding and investigating fraud.\n",
    "\n",
    "**Balancing False Positives and False Negatives:**\n",
    "\n",
    "In fraud detection:\n",
    "-   **False Positives (Type I Error):** Legitimate transactions flagged as fraud. This leads to customer inconvenience and potential churn. A high precision indicates fewer false positives.\n",
    "-   **False Negatives (Type II Error):** Actual fraudulent transactions missed. This leads to direct financial loss for the business. A high recall indicates fewer false negatives.\n",
    "\n",
    "For fraud detection, **Recall** is often prioritized to minimize financial losses, but **Precision** cannot be ignored to maintain customer experience. The **F1-score** provides a good balance between precision and recall. **AUC-PR** is particularly valuable as it focuses on the performance of the model on the positive (minority) class, giving a more realistic view of how well the model identifies fraud compared to AUC-ROC.\n",
    "\n",
    "XGBoost's ability to achieve a higher F1-score and AUC-PR, combined with its capacity for complex pattern recognition, makes it the more suitable and \"best\" model for this fraud detection task, even if its accuracy might be similar to Logistic Regression (as accuracy can be misleading on imbalanced data). The confusion matrix further helps in understanding the trade-offs between correctly identified fraud (True Positives) and missed fraud (False Negatives).\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps:**\n",
    "-   Further hyperparameter tuning for the XGBoost model using techniques like GridSearchCV or RandomizedSearchCV.\n",
    "-   Experiment with different class imbalance handling techniques (e.g., `scale_pos_weight` in XGBoost, different over/under-sampling methods).\n",
    "-   Consider ensembling multiple models or more advanced deep learning approaches for complex fraud patterns.\n",
    "-   Integrate MLflow for experiment tracking and model versioning (as hinted by `run_predict.py`).\n",
    "-   Deploy the best-performing model for real-time inference using `run_predict.py` or a similar serving mechanism.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}