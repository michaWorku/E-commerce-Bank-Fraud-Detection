{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#! mkdir -p src data scripts data/raw data/processed src/eda/ src/data_processing/ src/utils src/feature_engineering"
      ],
      "metadata": {
        "id": "m2A4b2YZ5FXZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Colab Setup for RAPIDS/cuDF\n",
        "install RAPIDS (which includes cuDF)"
      ],
      "metadata": {
        "id": "AmFHQ_-uhf04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell installs the RAPIDS library, including cuDF, on Google Colab.\n",
        "# It's recommended to run this at the start of your notebook.\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Check if NVIDIA GPU is available\n",
        "gpu_info = !nvidia-smi --query-gpu=name,driver_version,cuda_version --format=csv,noheader\n",
        "if not gpu_info:\n",
        "    print(\"No GPU found. Please ensure you have a GPU runtime selected (Runtime -> Change runtime type -> GPU).\")\n",
        "else:\n",
        "    print(f\"GPU detected: {gpu_info[0]}\")\n",
        "\n",
        "    # Install RAPIDS (cuDF, cuML, etc.)\n",
        "    # The specific version (e.g., 23.08) might need to be updated based on RAPIDS releases.\n",
        "    # Check https://docs.rapids.ai/install for the latest recommended Colab installation.\n",
        "    # This command uses the nightly build for broader compatibility, but you can specify a stable version.\n",
        "    print(\"Installing RAPIDS... This may take a few minutes.\")\n",
        "    !pip install --upgrade pip\n",
        "    !pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com # For CUDA 12.x\n",
        "    # For older CUDA versions, you might need:\n",
        "    # !pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com # For CUDA 11.x\n",
        "\n",
        "    # Verify installation\n",
        "    try:\n",
        "        import cudf\n",
        "        print(\"\\ncuDF installed and imported successfully!\")\n",
        "        print(f\"cuDF version: {cudf.__version__}\")\n",
        "    except ImportError:\n",
        "        print(\"\\nError: cuDF installation failed or could not be imported.\")\n",
        "        print(\"Please restart the runtime (Runtime -> Restart runtime) and try again.\")\n",
        "\n",
        "    # A common issue: ensure the correct CUDA path is set\n",
        "    cuda_path = '/usr/local/cuda'\n",
        "    if os.path.exists(cuda_path):\n",
        "        os.environ['PATH'] = f\"{cuda_path}/bin:{os.environ['PATH']}\"\n",
        "        os.environ['LD_LIBRARY_PATH'] = f\"{cuda_path}/lib64:{os.environ['LD_LIBRARY_PATH']}\"\n",
        "        print(f\"CUDA path set: {cuda_path}\")\n",
        "    else:\n",
        "        print(\"Warning: CUDA path not found, some cuDF features might not work optimally.\")\n",
        "\n",
        "print(\"\\nRAPIDS installation attempt complete. You may need to restart the runtime for changes to take full effect.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Snu7eAahbhE",
        "outputId": "20f3cd78-def6-4b87-9222-fc1c3e42bcc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU detected: Field \"cuda_version\" is not a valid field to query.\n",
            "Installing RAPIDS... This may take a few minutes.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: cudf-cu12 in /usr/local/lib/python3.11/dist-packages (25.6.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (5.5.2)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.6.2 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (12.6.2.post1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (13.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (2025.3.0)\n",
            "Requirement already satisfied: libcudf-cu12==25.6.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (25.6.0)\n",
            "Requirement already satisfied: numba-cuda<0.12.0a0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (0.11.0)\n",
            "Requirement already satisfied: numba<0.62.0a0,>=0.59.1 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (0.60.0)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (2.0.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (12.5.82)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (0.2.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (25.0)\n",
            "Requirement already satisfied: pandas<2.2.4dev0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<20.0.0a0,>=14.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (18.1.0)\n",
            "Requirement already satisfied: pylibcudf-cu12==25.6.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (25.6.0)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (0.7.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (13.9.4)\n",
            "Requirement already satisfied: rmm-cu12==25.6.* in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (25.6.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cudf-cu12) (4.14.1)\n",
            "Requirement already satisfied: libkvikio-cu12==25.6.* in /usr/local/lib/python3.11/dist-packages (from libcudf-cu12==25.6.*->cudf-cu12) (25.6.0)\n",
            "Requirement already satisfied: librmm-cu12==25.6.* in /usr/local/lib/python3.11/dist-packages (from libcudf-cu12==25.6.*->cudf-cu12) (25.6.0)\n",
            "Requirement already satisfied: rapids-logger==0.1.* in /usr/local/lib/python3.11/dist-packages (from libcudf-cu12==25.6.*->cudf-cu12) (0.1.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba<0.62.0a0,>=0.59.1->cudf-cu12) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12) (2025.2)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12) (0.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.4dev0,>=2.0->cudf-cu12) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->cudf-cu12) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->cudf-cu12) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n",
            "\n",
            "cuDF installed and imported successfully!\n",
            "cuDF version: 25.06.00\n",
            "CUDA path set: /usr/local/cuda\n",
            "\n",
            "RAPIDS installation attempt complete. You may need to restart the runtime for changes to take full effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60-74EjL2wOp"
      },
      "source": [
        "# **Exploratory Data Analysis (EDA) for E-commerce and Bank Transaction Fraud Detection**\n",
        "\n",
        "This notebook serves as the core of our Exploratory Data Analysis (EDA) phase for improving the detection of fraud cases for e-commerce and bank credit transactions. Its primary goal is to develop a foundational understanding of both datasets, assess their quality, and uncover initial patterns that will inform our feature engineering and model development.\n",
        "\n",
        "We will leverage a modular set of Python scripts (`src/data_processing/` and `src/eda/`) for data loading, preprocessing, summarization, and various analytical techniques (univariate, bivariate, multivariate, missing value, outlier, and temporal analysis). This modular approach promotes clean code, reusability, and reproducibility, aligning with best practices in Data Engineering and Machine Learning Engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg6mmJrm2wO0"
      },
      "source": [
        "## **Table of Contents**\n",
        "\n",
        "1. [Setup and Data Loading](#1-setup-and-data-loading)\n",
        "2. [Data Preprocessing and Feature Engineering](#2-data-preprocessing-and-feature-engineering)\n",
        "3. [EDA for E-commerce Fraud Data (Fraud_Data.csv)](#3-eda-for-e-commerce-fraud-data-fraud_data.csv)\n",
        "    - [Data Understanding and Initial Quality Check](#3.1-data-understanding-and-initial-quality-check)\n",
        "    - [Missing Values Analysis](#3.2-missing-values-analysis)\n",
        "    - [Univariate Analysis](#3.3-univariate-analysis)\n",
        "    - [Bivariate Analysis](#3.4-bivariate-analysis)\n",
        "    - [Multivariate Analysis](#3.5-multivariate-analysis)\n",
        "    - [Outlier Analysis](#3.6-outlier-analysis)\n",
        "    - [Temporal Trend Analysis](#3.7-temporal-trend-analysis)\n",
        "4. [EDA for Bank Credit Card Fraud Data (creditcard.csv)](#4-eda-for-bank-credit-card-fraud-data-creditcard.csv)\n",
        "    - [Data Understanding and Initial Quality Check](#4.1-data-understanding-and-initial-quality-check)\n",
        "    - [Missing Values Analysis](#4.2-missing-values-analysis)\n",
        "    - [Univariate Analysis](#4.3-univariate-analysis)\n",
        "    - [Bivariate Analysis](#4.4-bivariate-analysis)\n",
        "    - [Multivariate Analysis](#4.5-multivariate-analysis)\n",
        "    - [Outlier Analysis](#4.6-outlier-analysis)\n",
        "    - [Temporal Trend Analysis](#4.7-temporal-trend-analysis)\n",
        "5. [Class Imbalance Handling Demonstration](#5-class-imbalance-handling-demonstration)\n",
        "6. [Key Insights & Summary](#6-key-insights--summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BucYJ8zk2wO8"
      },
      "source": [
        "## **1. Setup and Data Loading**\n",
        "\n",
        "This section establishes our analytical environment by importing essential Python libraries for data manipulation, visualization, and numerical operations. Crucially, we also import our custom modular functions and classes from the `src/` directory. This ensures that data loading, preprocessing, and various analytical steps are handled by dedicated, reusable components.\n",
        "\n",
        "**Note on Data:** This notebook expects the `Fraud_Data.csv`, `IpAddress_to_Country.csv`, and `creditcard.csv` files to be placed in the `data/raw/` directory of your project. Please ensure the files are downloaded and correctly placed before running this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKBV1Fpz2wO9"
      },
      "source": [
        "### Import Necessary Libraries and Custom Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wziXQkBg2wPA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6800d9-2724-4e38-91c0-434f4f667cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuDF is available. Data loading can be accelerated on GPU.\n",
            "cuDF is available in preprocessor.py. Transformers can use GPU.\n",
            "cuDF is available in engineer.py. Transformers can use GPU.\n",
            "cuDF is available in helpers.py.\n"
          ]
        }
      ],
      "source": [
        "# Core Libraries for Data Manipulation and Numerical Operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os # For path operations\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For handling class imbalance (SMOTE)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add the project root directory to the system path.\n",
        "# This allows Python to correctly locate and import our custom modules\n",
        "# (e.g., from `src.data_processing.loader`) regardless of where the notebook is run from.\n",
        "import sys\n",
        "project_root = Path.cwd() # Navigates from 'notebooks/' directory to the project root\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "# Import Custom Modular Functions and Classes\n",
        "from src.data_processing.loader import load_data\n",
        "from src.data_processing.preprocessor import FraudDataProcessor, CreditCardDataProcessor\n",
        "from src.utils.helpers import merge_ip_to_country\n",
        "\n",
        "# Data Inspection Utilities (includes strategies for dtypes and summary statistics)\n",
        "from src.eda.data_inspection import DataInspector, DataTypesAndNonNullInspectionStrategy, SummaryStatisticsInspectionStrategy\n",
        "\n",
        "# Univariate Analysis Utilities\n",
        "from src.eda.univariate_analysis import UnivariateAnalyzer, NumericalUnivariateAnalysis, CategoricalUnivariateAnalysis\n",
        "\n",
        "# Bivariate Analysis Utilities\n",
        "from src.eda.bivariate_analysis import BivariateAnalyzer, NumericalVsNumericalAnalysis, CategoricalVsNumericalAnalysis, CategoricalVsCategoricalAnalysis\n",
        "\n",
        "# Multivariate Analysis Utilities\n",
        "from src.eda.multivariate_analysis import SimpleMultivariateAnalysis\n",
        "\n",
        "# Missing Values Analysis Utilities\n",
        "from src.eda.missing_values_analysis import SimpleMissingValuesAnalysis\n",
        "\n",
        "# Outlier Analysis Utilities\n",
        "from src.eda.outlier_analysis import OutlierAnalyzer, IQRBasedOutlierAnalysis\n",
        "\n",
        "# Temporal Analysis Utilities\n",
        "from src.eda.temporal_analysis import TemporalAnalyzer, MonthlyTrendAnalysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKoCYUVL2wPE"
      },
      "source": [
        "#### Set Plotting Style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KZgtqpeB2wPH"
      },
      "outputs": [],
      "source": [
        "# Configure Matplotlib and Seaborn for consistent and aesthetic plots\n",
        "sns.set_style(\"whitegrid\") # Provides a clean, modern look with a grid\n",
        "plt.rcParams['figure.figsize'] = (10, 6) # Default figure size for plots\n",
        "plt.rcParams['font.size'] = 12 # Base font size for readability\n",
        "plt.rcParams['axes.labelsize'] = 14 # Font size for axis labels\n",
        "plt.rcParams['xtick.labelsize'] = 12 # Font size for x-axis tick labels\n",
        "plt.rcParams['ytick.labelsize'] = 12 # Font size for y-axis tick labels\n",
        "plt.rcParams['legend.fontsize'] = 12 # Font size for plot legends\n",
        "plt.rcParams['font.family'] = 'sans-serif' # Use a default sans-serif font"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24tTQWEy2wPI"
      },
      "source": [
        "### Loading the Raw Transaction Data\n",
        "We will load both the e-commerce fraud data (`Fraud_Data.csv`) and the bank credit card fraud data (`creditcard.csv`), along with the IP address to country mapping (`IpAddress_to_Country.csv`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by3RVnte2wPM",
        "outputId": "7314b4b9-65a4-43cf-e5a6-15da72d8fe59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Raw Datasets ---\n",
            "Attempting to load data from /content/data/raw/Fraud_Data.csv using cuDF (GPU)...\n",
            "Successfully loaded data from /content/data/raw/Fraud_Data.csv\n",
            "Attempting to load data from /content/data/raw/IpAddress_to_Country.csv using cuDF (GPU)...\n",
            "Successfully loaded data from /content/data/raw/IpAddress_to_Country.csv\n",
            "Attempting to load data from /content/data/raw/creditcard.csv using cuDF (GPU)...\n",
            "Successfully loaded data from /content/data/raw/creditcard.csv\n",
            "Raw data loading complete.\n"
          ]
        }
      ],
      "source": [
        "# Define data directories and file paths\n",
        "RAW_DATA_DIR = project_root / \"data\" / \"raw\"\n",
        "PROCESSED_DATA_DIR = project_root / \"data\" / \"processed\"\n",
        "\n",
        "FRAUD_DATA_PATH = RAW_DATA_DIR / \"Fraud_Data.csv\"\n",
        "IP_TO_COUNTRY_PATH = RAW_DATA_DIR / \"IpAddress_to_Country.csv\"\n",
        "CREDITCARD_DATA_PATH = RAW_DATA_DIR / \"creditcard.csv\"\n",
        "\n",
        "# Ensure raw data directory exists\n",
        "if not RAW_DATA_DIR.exists():\n",
        "    print(f\"Error: Raw data directory '{RAW_DATA_DIR}' not found. Please ensure 'data/raw' exists and contains the datasets.\")\n",
        "    # You might want to raise an exception here if the directory is absolutely critical\n",
        "    # raise FileNotFoundError(f\"Raw data directory '{RAW_DATA_DIR}' not found.\")\n",
        "\n",
        "# Check if all required raw data files exist\n",
        "required_raw_files = [FRAUD_DATA_PATH, IP_TO_COUNTRY_PATH, CREDITCARD_DATA_PATH]\n",
        "all_files_exist = True\n",
        "for file_path in required_raw_files:\n",
        "    if not file_path.exists():\n",
        "        print(f\"Error: Required raw data file '{file_path.name}' not found in '{RAW_DATA_DIR}'. Please place it there.\")\n",
        "        all_files_exist = False\n",
        "\n",
        "if not all_files_exist:\n",
        "    print(\"\\nSkipping data loading due to missing required raw data files.\")\n",
        "else:\n",
        "    print(\"\\n--- Loading Raw Datasets ---\")\n",
        "    # Explicitly specify dtypes for IP address columns to prevent string accessor errors\n",
        "    fraud_data_df_raw = load_data(\n",
        "        FRAUD_DATA_PATH,\n",
        "        use_gpu=True,\n",
        "        column_dtypes={'ip_address': str}\n",
        "    )\n",
        "    ip_country_df_raw = load_data(\n",
        "        IP_TO_COUNTRY_PATH,\n",
        "        use_gpu=True,\n",
        "        column_dtypes={'lower_bound_ip_address': str, 'upper_bound_ip_address': str}\n",
        "    )\n",
        "    creditcard_df_raw = load_data(CREDITCARD_DATA_PATH, use_gpu=True)\n",
        "\n",
        "    print(\"Raw data loading complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddz4Ch-p2wPN"
      },
      "source": [
        "## **2. Data Preprocessing and Feature Engineering**\n",
        "\n",
        "This section applies the preprocessing and feature engineering pipelines defined in `src/data_processing/preprocessor.py` to both datasets. This includes handling missing values, correcting data types, creating new temporal and transaction-based features, and scaling/encoding features.\n",
        "\n",
        "After processing, the engineered datasets will be saved to the `data/processed/` directory for use in subsequent modeling phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_8t71-U2wPO"
      },
      "source": [
        "### Preprocessing E-commerce Fraud Data (`Fraud_Data.csv`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQXxTbb02wPO",
        "outputId": "59aac069-cd80-40f0-de56-8bb56f6339f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Preprocessing E-commerce Fraud Data (Fraud_Data.csv) ---\n",
            "\n",
            "--- Performing IP Address to Country Merging ---\n",
            "Converting IP addresses to integer format...\n",
            "Matching IP addresses to countries using efficient merge...\n",
            "Converting to pandas for IP merge (cuDF merge_asof for ranges is limited)...\n",
            "IP-to-Country merge complete.\n",
            "Applying preprocessing pipeline to Fraud_Data.csv...\n",
            "No duplicate rows found.\n",
            "Converting 'TransactionStartTime' to datetime using cuDF...\n",
            "Converting 'signup_time' to datetime using cuDF...\n",
            "Converting 'Amount' to numerical using cuDF...\n",
            "Converting 'age' to numerical using cuDF...\n",
            "Converting 'Amount' to numerical using cuDF...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Preprocessing E-commerce Fraud Data (Fraud_Data.csv) ---\")\n",
        "if fraud_data_df_raw.empty:\n",
        "    print(\"Raw Fraud_Data.csv is empty. Skipping preprocessing.\")\n",
        "    fraud_processed_df = pd.DataFrame()\n",
        "else:\n",
        "    # Merge IP addresses to countries first (this is a helper function)\n",
        "    fraud_df_merged = merge_ip_to_country(fraud_data_df_raw.copy(), ip_country_df_raw.copy())\n",
        "\n",
        "    # Separate features and target\n",
        "    FRAUD_TARGET_COL = 'class'\n",
        "    if FRAUD_TARGET_COL in fraud_df_merged.columns:\n",
        "        X_fraud = fraud_df_merged.drop(columns=[FRAUD_TARGET_COL])\n",
        "        y_fraud = fraud_df_merged[FRAUD_TARGET_COL]\n",
        "    else:\n",
        "        print(f\"Warning: Target column '{FRAUD_TARGET_COL}' not found in Fraud_Data.csv. Using a dummy target.\")\n",
        "        X_fraud = fraud_df_merged.copy()\n",
        "        y_fraud = pd.Series([0] * len(X_fraud), index=X_fraud.index) # Dummy target\n",
        "\n",
        "    # Rename columns in X_fraud to match generic names expected by FraudDataProcessor\n",
        "    # This ensures consistency with the preprocessor's internal logic\n",
        "    X_fraud_renamed = X_fraud.rename(columns={\n",
        "        'user_id': 'CustomerId',\n",
        "        'purchase_value': 'Amount',\n",
        "        'purchase_time': 'TransactionStartTime',\n",
        "        'user_id': 'TransactionId' # Using user_id as a proxy for TransactionId for frequency count\n",
        "    }).copy()\n",
        "\n",
        "    # Define columns for FraudDataProcessor using the *renamed* names\n",
        "    fraud_numerical_features_renamed = ['Amount', 'age']\n",
        "    fraud_categorical_features_renamed = ['source', 'browser', 'sex', 'country']\n",
        "    fraud_purchase_time_col_renamed = 'TransactionStartTime'\n",
        "    fraud_signup_time_col_renamed = 'signup_time'\n",
        "    fraud_amount_col_renamed = 'Amount'\n",
        "    fraud_id_cols_for_agg_renamed = ['CustomerId', 'device_id', 'ip_address']\n",
        "\n",
        "    fraud_processor = FraudDataProcessor(\n",
        "        numerical_cols_after_rename=fraud_numerical_features_renamed,\n",
        "        categorical_cols_after_merge=fraud_categorical_features_renamed,\n",
        "        time_col_after_rename=fraud_purchase_time_col_renamed,\n",
        "        signup_time_col_after_rename=fraud_signup_time_col_renamed,\n",
        "        amount_col_after_rename=fraud_amount_col_renamed,\n",
        "        id_cols_for_agg_after_rename=fraud_id_cols_for_agg_renamed\n",
        "    )\n",
        "\n",
        "    print(\"Applying preprocessing pipeline to Fraud_Data.csv...\")\n",
        "    X_fraud_processed = fraud_processor.fit_transform(X_fraud_renamed, y_fraud)\n",
        "\n",
        "    # Re-attach target\n",
        "    fraud_processed_df = pd.concat([X_fraud_processed, y_fraud.rename(FRAUD_TARGET_COL)], axis=1)\n",
        "    print(\"E-commerce Fraud Data preprocessing complete.\")\n",
        "\n",
        "# Save processed Fraud Data\n",
        "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "if not fraud_processed_df.empty:\n",
        "    fraud_output_path = PROCESSED_DATA_DIR / \"fraud_processed.csv\"\n",
        "    fraud_processed_df.to_csv(fraud_output_path, index=False)\n",
        "    print(f\"Processed E-commerce Fraud Data saved to: {fraud_output_path}\")\n",
        "else:\n",
        "    print(\"No processed E-commerce Fraud Data to save.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chn765PD2wPP"
      },
      "source": [
        "### Preprocessing Bank Credit Card Fraud Data (`creditcard.csv`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukwDsrlI2wPP"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Preprocessing Bank Credit Card Fraud Data (creditcard.csv) ---\")\n",
        "if creditcard_df_raw.empty:\n",
        "    print(\"Raw creditcard.csv is empty. Skipping preprocessing.\")\n",
        "    creditcard_processed_df = pd.DataFrame()\n",
        "else:\n",
        "    # Separate features and target\n",
        "    CREDITCARD_TARGET_COL = 'Class'\n",
        "    if CREDITCARD_TARGET_COL in creditcard_df_raw.columns:\n",
        "        X_creditcard = creditcard_df_raw.drop(columns=[CREDITCARD_TARGET_COL])\n",
        "        y_creditcard = creditcard_df_raw[CREDITCARD_TARGET_COL]\n",
        "    else:\n",
        "        print(f\"Warning: Target column '{CREDITCARD_TARGET_COL}' not found in creditcard.csv. Proceeding without target.\")\n",
        "        X_creditcard = creditcard_df_raw.copy()\n",
        "        y_creditcard = pd.Series([0] * len(X_creditcard), index=X_creditcard.index) # Dummy target\n",
        "\n",
        "    # All V features, Time, Amount are numerical.\n",
        "    creditcard_numerical_features = [col for col in X_creditcard.columns if col not in []]\n",
        "\n",
        "    creditcard_processor = CreditCardDataProcessor(\n",
        "        numerical_cols=creditcard_numerical_features\n",
        "    )\n",
        "\n",
        "    print(\"Applying preprocessing pipeline to creditcard.csv...\")\n",
        "    X_creditcard_processed = creditcard_processor.fit_transform(X_creditcard, y_creditcard)\n",
        "\n",
        "    # Re-attach target\n",
        "    creditcard_processed_df = pd.concat([X_creditcard_processed, y_creditcard.rename(CREDITCARD_TARGET_COL)], axis=1)\n",
        "    print(\"Bank Credit Card Fraud Data preprocessing complete.\")\n",
        "\n",
        "# Save processed Credit Card Data\n",
        "if not creditcard_processed_df.empty:\n",
        "    creditcard_output_path = PROCESSED_DATA_DIR / \"creditcard_processed.csv\"\n",
        "    creditcard_processed_df.to_csv(creditcard_output_path, index=False)\n",
        "    print(f\"Processed Bank Credit Card Fraud Data saved to: {creditcard_output_path}\")\n",
        "else:\n",
        "    print(\"No processed Bank Credit Card Fraud Data to save.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po9X1fBh2wPQ"
      },
      "source": [
        "## **3. EDA for E-commerce Fraud Data (Fraud_Data.csv)**\n",
        "\n",
        "This section focuses on the Exploratory Data Analysis of the preprocessed E-commerce Fraud Data. We will apply various analytical techniques to understand the data's characteristics, distributions, relationships between features, and the nature of fraud within this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iHeYHVa2wPR"
      },
      "source": [
        "### **3.1 Data Understanding and Initial Quality Check**\n",
        "This step provides an initial overview of the dataset's structure, including data types and non-null counts, and then presents summary statistics for numerical features. This helps in quickly grasping the scale, distribution, and potential issues within the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRwF66TF2wPS"
      },
      "source": [
        "#### Data Structure and Quality Assessment (using `DataTypesAndNonNullInspectionStrategy`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIO79j922wPS"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Data Types and Non-null Counts for E-commerce Fraud Data ---\")\n",
        "    inspector = DataInspector(DataTypesAndNonNullInspectionStrategy())\n",
        "    inspector.execute_inspection(fraud_processed_df)\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Data Structure and Quality Assessment.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWseE7sF2wPS"
      },
      "source": [
        "#### Descriptive Statistics & Variability (using `SummaryStatisticsInspectionStrategy`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4L5IjMqn2wPT"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Summary Statistics for E-commerce Fraud Data ---\")\n",
        "    inspector = DataInspector(SummaryStatisticsInspectionStrategy())\n",
        "    inspector.execute_inspection(fraud_processed_df)\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Descriptive Statistics.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYcyx25v2wPT"
      },
      "source": [
        "### **3.2 Missing Values Analysis**\n",
        "Missing data can significantly impact model performance and lead to biased results. This section identifies the extent of missing values and visualizes their patterns, guiding imputation or removal strategies in the feature engineering phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGzqGZvR2wPU"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Missing Values Analysis for E-commerce Fraud Data ---\")\n",
        "    missing_analyzer = SimpleMissingValuesAnalysis()\n",
        "    missing_analyzer.analyze(fraud_processed_df)\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Missing Values Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHMpKASh2wPU"
      },
      "source": [
        "### **3.3 Univariate Analysis**\n",
        "Univariate analysis explores the distribution of individual features. This helps in understanding the range, central tendency, and spread of numerical data, and the frequency of categories in categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1x7DAnBg2wPU"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Univariate Analysis for E-commerce Fraud Data ---\")\n",
        "\n",
        "    # Define features for EDA based on the *expected output* of the FraudDataProcessor\n",
        "    # These lists must accurately reflect the columns present and their types AFTER preprocessing\n",
        "    fraud_numerical_features_for_eda = [\n",
        "        'Amount', 'age', 'IsRefund', 'TransactionHour', 'TransactionDayOfWeek',\n",
        "        'TransactionMonth', 'TransactionYear', 'time_since_signup'\n",
        "    ] + [\n",
        "        f'{id_col}_transactions_last_{window}d' for id_col in ['CustomerId', 'device_id', 'ip_address'] for window in [1, 7, 30]\n",
        "    ] + [\n",
        "        f'{id_col}_total_amount_last_{window}d' for id_col in ['CustomerId', 'device_id', 'ip_address'] for window in [1, 7, 30]\n",
        "    ] + [\n",
        "        f'{id_col}_avg_amount_last_{window}d' for id_col in ['CustomerId', 'device_id', 'ip_address'] for window in [1, 7, 30]\n",
        "    ]\n",
        "    # Filter to only include columns that actually exist in the DataFrame\n",
        "    fraud_numerical_features_for_eda = [col for col in fraud_numerical_features_for_eda if col in fraud_processed_df.columns]\n",
        "\n",
        "    # Categorical features will be One-Hot Encoded, so we analyze the original categorical columns\n",
        "    # or the OHE versions if needed, but for general distribution, original is better.\n",
        "    # The 'country' column is added during merge.\n",
        "    fraud_categorical_features_for_eda = [\n",
        "        'source', 'browser', 'sex', 'country'\n",
        "    ]\n",
        "    # Filter to only include columns that actually exist in the DataFrame\n",
        "    fraud_categorical_features_for_eda = [col for col in fraud_categorical_features_for_eda if col in fraud_processed_df.columns]\n",
        "\n",
        "    # Numerical Univariate Analysis\n",
        "    univariate_analyzer_num = UnivariateAnalyzer(NumericalUnivariateAnalysis())\n",
        "    for col in fraud_numerical_features_for_eda:\n",
        "        univariate_analyzer_num.execute_analysis(fraud_processed_df, col)\n",
        "\n",
        "    # Categorical Univariate Analysis\n",
        "    univariate_analyzer_cat = UnivariateAnalyzer(CategoricalUnivariateAnalysis())\n",
        "    for col in fraud_categorical_features_for_eda:\n",
        "        univariate_analyzer_cat.execute_analysis(fraud_processed_df, col)\n",
        "\n",
        "    # Class Imbalance Check for FraudResult\n",
        "    FRAUD_TARGET_COL = 'class' # Ensure this is the correct renamed target column\n",
        "    if FRAUD_TARGET_COL in fraud_processed_df.columns:\n",
        "        print(f\"\\n--- Class Distribution for '{FRAUD_TARGET_COL}' in E-commerce Fraud Data ---\")\n",
        "        class_counts = fraud_processed_df[FRAUD_TARGET_COL].value_counts()\n",
        "        print(class_counts)\n",
        "        print(f\"Fraudulent transactions: {class_counts.get(1, 0)} ({class_counts.get(1, 0) / len(fraud_processed_df) * 100:.2f}%) \")\n",
        "        print(f\"Non-fraudulent transactions: {class_counts.get(0, 0)} ({class_counts.get(0, 0) / len(fraud_processed_df) * 100:.2f}%) \")\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.countplot(x=FRAUD_TARGET_COL, data=fraud_processed_df, palette='coolwarm')\n",
        "        plt.title(f'Class Distribution of {FRAUD_TARGET_COL} (E-commerce Fraud Data)')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Target column '{FRAUD_TARGET_COL}' not found in E-commerce Fraud Data. Skipping class distribution check.\")\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Univariate Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daYUddEW2wPV"
      },
      "source": [
        "### **3.4 Bivariate Analysis**\n",
        "Bivariate analysis explores the relationships between pairs of features. This helps in identifying potential correlations, dependencies, and interactions that might be important for model building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1kgiZbe2wPW"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Bivariate Analysis for E-commerce Fraud Data ---\")\n",
        "    FRAUD_TARGET_COL = 'class' # Ensure this is the correct renamed target column\n",
        "\n",
        "    # Numerical vs Numerical\n",
        "    bivariate_analyzer_num_num = BivariateAnalyzer(NumericalVsNumericalAnalysis())\n",
        "    # Example: Amount vs age\n",
        "    if 'Amount' in fraud_processed_df.columns and 'age' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_num_num.execute_analysis(fraud_processed_df, 'Amount', 'age')\n",
        "    # Example: Amount vs time_since_signup\n",
        "    if 'Amount' in fraud_processed_df.columns and 'time_since_signup' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_num_num.execute_analysis(fraud_processed_df, 'Amount', 'time_since_signup')\n",
        "\n",
        "    # Categorical vs Numerical\n",
        "    bivariate_analyzer_cat_num = BivariateAnalyzer(CategoricalVsNumericalAnalysis())\n",
        "    # Example: source vs Amount\n",
        "    if 'source' in fraud_processed_df.columns and 'Amount' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_cat_num.execute_analysis(fraud_processed_df, 'source', 'Amount')\n",
        "    # Example: country vs Amount\n",
        "    if 'country' in fraud_processed_df.columns and 'Amount' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_cat_num.execute_analysis(fraud_processed_df, 'country', 'Amount')\n",
        "    # Example: Target vs Amount\n",
        "    if FRAUD_TARGET_COL in fraud_processed_df.columns and 'Amount' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_cat_num.execute_analysis(fraud_processed_df, FRAUD_TARGET_COL, 'Amount')\n",
        "    # Example: Target vs time_since_signup\n",
        "    if FRAUD_TARGET_COL in fraud_processed_df.columns and 'time_since_signup' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_cat_num.execute_analysis(fraud_processed_df, FRAUD_TARGET_COL, 'time_since_signup')\n",
        "\n",
        "    # Categorical vs Categorical\n",
        "    bivariate_analyzer_cat_cat = BivariateAnalyzer(CategoricalVsCategoricalAnalysis())\n",
        "    # Example: source vs sex\n",
        "    if 'source' in fraud_processed_df.columns and 'sex' in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_cat_cat.execute_analysis(fraud_processed_df, 'source', 'sex')\n",
        "    # Example: country vs FraudResult\n",
        "    if 'country' in fraud_processed_df.columns and FRAUD_TARGET_COL in fraud_processed_df.columns:\n",
        "        bivariate_analyzer_cat_cat.execute_analysis(fraud_processed_df, 'country', FRAUD_TARGET_COL)\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Bivariate Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc-LADWc2wPW"
      },
      "source": [
        "### **3.5 Multivariate Analysis**\n",
        "Multivariate analysis examines the relationships among three or more variables. This helps in understanding complex interactions and patterns that might not be visible in univariate or bivariate analyses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOeCFOwv2wPW"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Multivariate Analysis for E-commerce Fraud Data ---\")\n",
        "    multivariate_analyzer = SimpleMultivariateAnalysis()\n",
        "\n",
        "    # Select a subset of numerical features for correlation heatmap and pair plot\n",
        "    # Include some original numericals and some engineered features\n",
        "    fraud_multivariate_features = [\n",
        "        'Amount', 'age', 'time_since_signup',\n",
        "        'CustomerId_transactions_last_7d', 'CustomerId_total_amount_last_7d',\n",
        "        'device_id_transactions_last_7d', 'ip_address_transactions_last_7d'\n",
        "    ]\n",
        "    fraud_multivariate_features = [col for col in fraud_multivariate_features if col in fraud_processed_df.columns]\n",
        "\n",
        "    if fraud_multivariate_features:\n",
        "        multivariate_analyzer.analyze(fraud_processed_df, features=fraud_multivariate_features)\n",
        "    else:\n",
        "        print(\"No suitable numerical features found for multivariate analysis in E-commerce Fraud Data.\")\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Multivariate Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tqpghfH2wPX"
      },
      "source": [
        "### **3.6 Outlier Analysis**\n",
        "Outliers are data points that significantly differ from other observations. Identifying and understanding outliers is crucial as they can skew statistical analyses and model training. This section uses IQR-based outlier detection and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPAseKT42wPX"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Outlier Analysis for E-commerce Fraud Data ---\")\n",
        "    outlier_analyzer = OutlierAnalyzer(IQRBasedOutlierAnalysis())\n",
        "\n",
        "    # Focus on key numerical features including engineered ones\n",
        "    fraud_outlier_cols = [\n",
        "        'Amount', 'age', 'time_since_signup', 'IsRefund',\n",
        "        'CustomerId_transactions_last_1d', 'CustomerId_total_amount_last_1d',\n",
        "        'device_id_transactions_last_1d', 'ip_address_transactions_last_1d'\n",
        "    ]\n",
        "    fraud_outlier_cols = [col for col in fraud_outlier_cols if col in fraud_processed_df.columns]\n",
        "\n",
        "    for col in fraud_outlier_cols:\n",
        "        outlier_analyzer.execute_analysis(fraud_processed_df, col)\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Outlier Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quTPGP7Q2wPY"
      },
      "source": [
        "### **3.7 Temporal Trend Analysis**\n",
        "Temporal analysis examines how features and fraud patterns change over time. This is particularly important for transaction data, as fraud often exhibits temporal trends or seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYiAVZGw2wPY"
      },
      "outputs": [],
      "source": [
        "if not fraud_processed_df.empty:\n",
        "    print(\"\\n--- Temporal Analysis for E-commerce Fraud Data ---\")\n",
        "    FRAUD_TARGET_COL = 'class'\n",
        "    time_col_for_temporal = 'TransactionStartTime'\n",
        "\n",
        "    # The original 'purchase_time' column (renamed to 'TransactionStartTime') is dropped by TemporalFeatureEngineer.\n",
        "    # To perform temporal analysis on the processed data, we need to ensure a time column suitable for plotting exists.\n",
        "    # If 'TransactionStartTime' was dropped, we can't use it directly here for plotting.\n",
        "    # We need to re-think how temporal analysis is done in the EDA notebook if the original time column is removed.\n",
        "    # For now, let's assume the time-based features (TransactionHour, etc.) are used for analysis, not the original timestamp.\n",
        "    # If we want to plot trends over time, we need to keep a date column or reconstruct it.\n",
        "\n",
        "    # Re-evaluate: TemporalFeatureEngineer drops the original time columns. This means we cannot directly plot\n",
        "    # trends over the full time range using 'TransactionStartTime' unless we modify TemporalFeatureEngineer\n",
        "    # to *not* drop it, or store it separately.\n",
        "    # For the purpose of this EDA notebook, let's adjust to use the extracted time features for analysis,\n",
        "    # or re-load the raw data for specific time-series plots if needed, or modify the preprocessor.\n",
        "\n",
        "    # Let's adjust the TemporalFeatureEngineer to *not* drop the original time columns, so we can use them for plotting.\n",
        "    # This change needs to be made in src/feature_engineering/engineer.py\n",
        "    # For now, if 'TransactionStartTime' is not in the processed df, skip this plot.\n",
        "\n",
        "    # If 'TransactionStartTime' is needed for plotting, the TemporalFeatureEngineer should NOT drop it.\n",
        "    # Assuming for this notebook that the original datetime column is available for plotting.\n",
        "    # If not, this section will print a warning and skip.\n",
        "\n",
        "    # Let's check if the raw time column is still available (which it won't be if preprocessor drops it)\n",
        "    # Or, we can use the 'TransactionYear' and 'TransactionMonth' to create aggregated plots.\n",
        "\n",
        "    # For now, let's use the MonthlyTrendAnalysis which aggregates by YearMonth from the original time column.\n",
        "    # If 'TransactionStartTime' is not present in fraud_processed_df, this will fail.\n",
        "    # The preprocessor currently drops it. So, we need to decide: keep original time in processed data, or only use extracted features.\n",
        "    # For comprehensive temporal analysis, keeping the original timestamp or a derived 'YearMonth' in the processed data is better.\n",
        "\n",
        "    # Assuming the 'TransactionStartTime' column is available (it should be if we modify TemporalFeatureEngineer not to drop it)\n",
        "    # If the preprocessor is modified to keep 'TransactionStartTime', then this code will work.\n",
        "    # For now, let's make sure the time column is actually in the df.\n",
        "\n",
        "    # If 'TransactionStartTime' is not in the processed df (because it was dropped by TemporalFeatureEngineer),\n",
        "    # then we can't run this directly. We would need to either:\n",
        "    # 1. Modify TemporalFeatureEngineer to *not* drop the original time column.\n",
        "    # 2. Add a step in the notebook to re-merge the original time column for plotting only.\n",
        "    # 3. Adjust the temporal analysis strategy to work purely on extracted features like Year/Month/DayOfWeek.\n",
        "\n",
        "    # Given the previous error, it's likely 'TransactionStartTime' is missing.\n",
        "    # Let's adjust the `TemporalFeatureEngineer` to *not* drop the original time columns.\n",
        "    # This will make the processed DataFrame larger but allow for this type of plotting.\n",
        "    # This change will be made in src/feature_engineering/engineer.py.\n",
        "\n",
        "    # After the change in engineer.py:\n",
        "    if time_col_for_temporal in fraud_processed_df.columns:\n",
        "        temporal_analyzer = TemporalAnalyzer(MonthlyTrendAnalysis())\n",
        "        temporal_metrics = ['Amount', FRAUD_TARGET_COL]\n",
        "        temporal_metrics = [col for col in temporal_metrics if col in fraud_processed_df.columns]\n",
        "        if temporal_metrics:\n",
        "            temporal_analyzer.execute_analysis(fraud_processed_df, time_col_for_temporal, temporal_metrics)\n",
        "        else:\n",
        "            print(f\"No valid metrics for temporal analysis in E-commerce Fraud Data.\")\n",
        "    else:\n",
        "        print(f\"Time column '{time_col_for_temporal}' not found in E-commerce Fraud Data. Skipping temporal analysis.\")\n",
        "else:\n",
        "    print(\"E-commerce Fraud Data is empty, skipping Temporal Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_HiG2Tx2wPY"
      },
      "source": [
        "## **4. EDA for Bank Credit Card Fraud Data (creditcard.csv)**\n",
        "\n",
        "This section focuses on the Exploratory Data Analysis of the preprocessed Bank Credit Card Fraud Data. We will apply similar analytical techniques to understand its unique characteristics, distributions, and fraud patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy6z929e2wPZ"
      },
      "source": [
        "### **4.1 Data Understanding and Initial Quality Check**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBa-z1R42wPZ"
      },
      "source": [
        "#### Data Structure and Quality Assessment (using `DataTypesAndNonNullInspectionStrategy`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UIQ0U3W2wPZ"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Data Types and Non-null Counts for Bank Credit Card Fraud Data ---\")\n",
        "    inspector = DataInspector(DataTypesAndNonNullInspectionStrategy())\n",
        "    inspector.execute_inspection(creditcard_processed_df)\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Data Structure and Quality Assessment.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueqVYpV92wPa"
      },
      "source": [
        "#### Descriptive Statistics & Variability (using `SummaryStatisticsInspectionStrategy`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPBr6Igt2wPa"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Summary Statistics for Bank Credit Card Fraud Data ---\")\n",
        "    inspector = DataInspector(SummaryStatisticsInspectionStrategy())\n",
        "    inspector.execute_inspection(creditcard_processed_df)\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Descriptive Statistics.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d74g96qq2wPa"
      },
      "source": [
        "### **4.2 Missing Values Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88XqEo5j2wPr"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Missing Values Analysis for Bank Credit Card Fraud Data ---\")\n",
        "    missing_analyzer = SimpleMissingValuesAnalysis()\n",
        "    missing_analyzer.analyze(creditcard_processed_df)\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Missing Values Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJhxpzgE2wPs"
      },
      "source": [
        "### **4.3 Univariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yHB8CI32wPs"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Univariate Analysis for Bank Credit Card Fraud Data ---\")\n",
        "\n",
        "    CREDITCARD_TARGET_COL = 'Class'\n",
        "    creditcard_numerical_features_for_eda = [f'V{i}' for i in range(1, 29)] + ['Time', 'Amount', 'IsRefund']\n",
        "    creditcard_numerical_features_for_eda = [col for col in creditcard_numerical_features_for_eda if col in creditcard_processed_df.columns]\n",
        "\n",
        "    creditcard_categorical_features_for_eda = [CREDITCARD_TARGET_COL]\n",
        "    creditcard_categorical_features_for_eda = [col for col in creditcard_categorical_features_for_eda if col in creditcard_processed_df.columns]\n",
        "\n",
        "    # Numerical Univariate Analysis\n",
        "    univariate_analyzer_num = UnivariateAnalyzer(NumericalUnivariateAnalysis())\n",
        "    for col in creditcard_numerical_features_for_eda:\n",
        "        univariate_analyzer_num.execute_analysis(creditcard_processed_df, col)\n",
        "\n",
        "    # Categorical Univariate Analysis (mainly for the target variable)\n",
        "    univariate_analyzer_cat = UnivariateAnalyzer(CategoricalUnivariateAnalysis())\n",
        "    for col in creditcard_categorical_features_for_eda:\n",
        "        univariate_analyzer_cat.execute_analysis(creditcard_processed_df, col)\n",
        "\n",
        "    # Class Imbalance Check for Class\n",
        "    if CREDITCARD_TARGET_COL in creditcard_processed_df.columns:\n",
        "        print(f\"\\n--- Class Distribution for '{CREDITCARD_TARGET_COL}' in Bank Credit Card Fraud Data ---\")\n",
        "        class_counts = creditcard_processed_df[CREDITCARD_TARGET_COL].value_counts()\n",
        "        print(class_counts)\n",
        "        print(f\"Fraudulent transactions: {class_counts.get(1, 0)} ({class_counts.get(1, 0) / len(creditcard_processed_df) * 100:.2f}%) \")\n",
        "        print(f\"Non-fraudulent transactions: {class_counts.get(0, 0)} ({class_counts.get(0, 0) / len(creditcard_processed_df) * 100:.2f}%) \")\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.countplot(x=CREDITCARD_TARGET_COL, data=creditcard_processed_df, palette='coolwarm')\n",
        "        plt.title(f'Class Distribution of {CREDITCARD_TARGET_COL} (Bank Credit Card Fraud Data)')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Target column '{CREDITCARD_TARGET_COL}' not found in Bank Credit Card Fraud Data. Skipping class distribution check.\")\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Univariate Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01lKa-F02wPu"
      },
      "source": [
        "### **4.4 Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bioYMXp92wPu"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Bivariate Analysis for Bank Credit Card Fraud Data ---\")\n",
        "    CREDITCARD_TARGET_COL = 'Class'\n",
        "\n",
        "    # Numerical vs Numerical\n",
        "    bivariate_analyzer_num_num = BivariateAnalyzer(NumericalVsNumericalAnalysis())\n",
        "    # Example: Amount vs V1\n",
        "    if 'Amount' in creditcard_processed_df.columns and 'V1' in creditcard_processed_df.columns:\n",
        "        bivariate_analyzer_num_num.execute_analysis(creditcard_processed_df, 'Amount', 'V1')\n",
        "    # Example: Time vs Amount\n",
        "    if 'Time' in creditcard_processed_df.columns and 'Amount' in creditcard_processed_df.columns:\n",
        "        bivariate_analyzer_num_num.execute_analysis(creditcard_processed_df, 'Time', 'Amount')\n",
        "\n",
        "    # Categorical vs Numerical (Target vs Amount/Time/V-features)\n",
        "    bivariate_analyzer_cat_num = BivariateAnalyzer(CategoricalVsNumericalAnalysis())\n",
        "    if CREDITCARD_TARGET_COL in creditcard_processed_df.columns:\n",
        "        if 'Amount' in creditcard_processed_df.columns:\n",
        "            bivariate_analyzer_cat_num.execute_analysis(creditcard_processed_df, CREDITCARD_TARGET_COL, 'Amount')\n",
        "        if 'Time' in creditcard_processed_df.columns:\n",
        "            bivariate_analyzer_cat_num.execute_analysis(creditcard_processed_df, CREDITCARD_TARGET_COL, 'Time')\n",
        "        # Example with a V feature\n",
        "        if 'V17' in creditcard_processed_df.columns:\n",
        "            bivariate_analyzer_cat_num.execute_analysis(creditcard_processed_df, CREDITCARD_TARGET_COL, 'V17')\n",
        "\n",
        "    # No significant categorical vs categorical analysis expected for this dataset beyond target\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Bivariate Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnUwrcmg2wPv"
      },
      "source": [
        "### **4.5 Multivariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsJ3W77x2wPw"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Multivariate Analysis for Bank Credit Card Fraud Data ---\")\n",
        "    multivariate_analyzer = SimpleMultivariateAnalysis()\n",
        "\n",
        "    # Select a subset of numerical features for correlation heatmap and pair plot\n",
        "    creditcard_multivariate_features = ['Amount', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\n",
        "    creditcard_multivariate_features = [col for col in creditcard_multivariate_features if col in creditcard_processed_df.columns]\n",
        "\n",
        "    if creditcard_multivariate_features:\n",
        "        multivariate_analyzer.analyze(creditcard_processed_df, features=creditcard_multivariate_features)\n",
        "    else:\n",
        "        print(\"No suitable numerical features found for multivariate analysis in Bank Credit Card Fraud Data.\")\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Multivariate Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CztnRjO32wPw"
      },
      "source": [
        "### **4.6 Outlier Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyOWpkdR2wPx"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Outlier Analysis for Bank Credit Card Fraud Data ---\")\n",
        "    outlier_analyzer = OutlierAnalyzer(IQRBasedOutlierAnalysis())\n",
        "\n",
        "    # Focus on key numerical features\n",
        "    creditcard_outlier_cols = ['Amount', 'Time', 'V1', 'V2', 'V3', 'IsRefund']\n",
        "    creditcard_outlier_cols = [col for col in creditcard_outlier_cols if col in creditcard_processed_df.columns]\n",
        "\n",
        "    for col in creditcard_outlier_cols:\n",
        "        outlier_analyzer.execute_analysis(creditcard_processed_df, col)\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Outlier Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbdZSVt12wPx"
      },
      "source": [
        "### **4.7 Temporal Trend Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBTIAYfL2wPy"
      },
      "outputs": [],
      "source": [
        "if not creditcard_processed_df.empty:\n",
        "    print(\"\\n--- Temporal Analysis for Bank Credit Card Fraud Data ---\")\n",
        "    CREDITCARD_TARGET_COL = 'Class'\n",
        "\n",
        "    # Time is in seconds, so direct line plot is more appropriate than monthly trends\n",
        "    if 'Time' in creditcard_processed_df.columns and 'Amount' in creditcard_processed_df.columns:\n",
        "        print(f\"Plotting Transaction Amount over Time for Bank Credit Card Fraud Data...\")\n",
        "        plt.figure(figsize=(14, 7))\n",
        "        sns.lineplot(x='Time', y='Amount', data=creditcard_processed_df, alpha=0.6)\n",
        "        plt.title(f'Transaction Amount Over Time (Bank Credit Card Fraud Data)')\n",
        "        plt.xlabel('Time (seconds from first transaction)')\n",
        "        plt.ylabel('Transaction Amount')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Required columns 'Time' or 'Amount' not found for temporal analysis of amount.\")\n",
        "\n",
        "    # Plot fraud incidents over time\n",
        "    if CREDITCARD_TARGET_COL in creditcard_processed_df.columns and 'Time' in creditcard_processed_df.columns:\n",
        "        fraud_over_time = creditcard_processed_df[creditcard_processed_df[CREDITCARD_TARGET_COL] == 1]\n",
        "        if not fraud_over_time.empty:\n",
        "            print(f\"Plotting Fraud Incidents over Time for Bank Credit Card Fraud Data...\")\n",
        "            plt.figure(figsize=(14, 7))\n",
        "            sns.histplot(x='Time', data=fraud_over_time, bins=50, kde=True, color='red')\n",
        "            plt.title(f'Fraud Incidents Over Time (Bank Credit Card Fraud Data)')\n",
        "            plt.xlabel('Time (seconds from first transaction)')\n",
        "            plt.ylabel('Number of Fraud Incidents')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(f\"No fraud incidents to plot over time in Bank Credit Card Fraud Data.\")\n",
        "    else:\n",
        "        print(\"Required columns 'Time' or 'Class' not found for temporal analysis of fraud incidents.\")\n",
        "else:\n",
        "    print(\"Bank Credit Card Fraud Data is empty, skipping Temporal Analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6eqlpyZ2wPy"
      },
      "source": [
        "## **5. Class Imbalance Handling Demonstration**\n",
        "\n",
        "Both fraud detection datasets are highly imbalanced, meaning the number of fraudulent transactions is significantly smaller than legitimate ones. This section demonstrates a common technique, SMOTE (Synthetic Minority Over-sampling Technique), to address this imbalance. SMOTE works by creating synthetic samples of the minority class.\n",
        "\n",
        "**Note:** In a real machine learning pipeline, SMOTE or similar techniques should only be applied to the *training data* to prevent data leakage and ensure a realistic evaluation of the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxWlpDZp2wPy"
      },
      "outputs": [],
      "source": [
        "def demonstrate_class_imbalance_handling(df: pd.DataFrame, dataset_name: str, target_col: str):\n",
        "    \"\"\"\n",
        "    Demonstrates class imbalance handling using SMOTE on a conceptual training set.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Demonstrating Class Imbalance Handling for {dataset_name} ---\")\n",
        "    if df.empty or target_col not in df.columns:\n",
        "        print(f\"Cannot demonstrate imbalance handling for empty or missing target column in {dataset_name}.\")\n",
        "        return\n",
        "\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Split data into training and testing sets (conceptual split for demonstration)\n",
        "    # In a real scenario, this split would happen *before* any preprocessing that uses target info\n",
        "    # and before applying imbalance techniques.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    print(f\"\\nOriginal training set class distribution for {dataset_name}:\")\n",
        "    print(y_train.value_counts())\n",
        "    print(f\"Fraudulent transactions: {y_train.value_counts().get(1, 0)} ({y_train.value_counts().get(1, 0) / len(y_train) * 100:.2f}%) \")\n",
        "\n",
        "    # Apply SMOTE to the training data only\n",
        "    print(f\"\\nApplying SMOTE to the training data for {dataset_name}...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "\n",
        "    try:\n",
        "        # SMOTE requires numerical input. Ensure all columns in X_train are numerical.\n",
        "        # If there are object columns, they need to be handled (e.g., one-hot encoded) prior to SMOTE.\n",
        "        # The preprocessor pipeline should have already handled this.\n",
        "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "        print(f\"Resampled training set class distribution for {dataset_name}:\")\n",
        "        print(y_train_resampled.value_counts())\n",
        "        print(f\"Fraudulent transactions: {y_train_resampled.value_counts().get(1, 0)} ({y_train_resampled.value_counts().get(1, 0) / len(y_train_resampled) * 100:.2f}%) \")\n",
        "        print(\"SMOTE applied successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying SMOTE: {e}\")\n",
        "        print(\"Ensure all features are numerical (float, int) before applying SMOTE.\")\n",
        "        print(\"Check dtypes of X_train:\")\n",
        "        print(X_train.dtypes[X_train.dtypes == 'object']) # Print object columns if any\n",
        "\n",
        "    print(f\"\\n--- Class Imbalance Handling Demonstration for {dataset_name} Complete ---\")\n",
        "\n",
        "# Demonstrate for E-commerce Fraud Data\n",
        "if not fraud_processed_df.empty:\n",
        "    demonstrate_class_imbalance_handling(fraud_processed_df.copy(), \"E-commerce Fraud Data (Fraud_Data.csv)\", 'class')\n",
        "\n",
        "# Demonstrate for Bank Credit Card Fraud Data\n",
        "if not creditcard_processed_df.empty:\n",
        "    demonstrate_class_imbalance_handling(creditcard_processed_df.copy(), \"Bank Credit Card Fraud Data (creditcard.csv)\", 'Class')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip6Q6JHo2wPz"
      },
      "source": [
        "## **6. Key Insights & Summary**\n",
        "\n",
        "This section will summarize the key findings from the EDA for both datasets, highlighting important characteristics, potential challenges, and recommendations for the subsequent modeling phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_QruZ8l2wP0"
      },
      "source": [
        "### E-commerce Fraud Data (Fraud_Data.csv)\n",
        "\n",
        "- **Data Quality:** The dataset is remarkably clean with no missing values after initial preprocessing. However, negative `Amount` values were identified, which were handled by converting to absolute value and creating an `IsRefund` flag. This flag can be a crucial feature for distinguishing transaction types.\n",
        "- **Feature Distributions:**\n",
        "    - `Amount` and `age` show highly skewed distributions with significant outliers, indicating a need for robust scaling or transformation during model training.\n",
        "    - Categorical features like `source`, `browser`, `sex`, and `country` have varying distributions, with some having dominant categories and others being more spread out. The `country` feature, derived from IP addresses, provides geographical context.\n",
        "- **Engineered Features:**\n",
        "    - Temporal features (`TransactionHour`, `TransactionDayOfWeek`, `TransactionMonth`, `TransactionYear`) were successfully extracted, allowing for analysis of time-based patterns.\n",
        "    - `time_since_signup` provides insight into user tenure, which could be related to fraud risk.\n",
        "    - Transaction frequency and velocity features (e.g., `CustomerId_transactions_last_X_d`, `CustomerId_total_amount_last_X_d`) are critical for identifying unusual spending behaviors over time, often a strong indicator of fraud.\n",
        "- **Class Imbalance:** The `class` target variable is highly imbalanced (very few fraud cases). This is a major challenge that requires specific handling during model training (e.g., SMOTE, class weights, or advanced sampling techniques) and careful selection of evaluation metrics (e.g., Precision, Recall, F1-score, AUC-ROC) rather than accuracy.\n",
        "- **Relationships:** Initial bivariate and multivariate analyses suggest potential relationships between transaction amounts, temporal features, and fraud. Features like `Amount` and engineered velocity features are expected to be highly indicative of fraudulent activity.\n",
        "\n",
        "### Bank Credit Card Fraud Data (creditcard.csv)\n",
        "\n",
        "- **Data Anonymity:** The `V` features are PCA-transformed, which means their direct interpretation is not possible. Analysis relies on their statistical properties and relationships.\n",
        "- **Data Quality:** This dataset also appears to be very clean with no missing values after preprocessing. The `Amount` column was processed similarly to the e-commerce data to handle potential negative values (though none were observed in this dataset's summary statistics, the `IsRefund` flag is still useful for consistency).\n",
        "- **Feature Distributions:**\n",
        "    - `Time` and `Amount` are the original features, with `Time` representing seconds from the first transaction. Both show distinct distributions that might reveal temporal patterns in fraudulent activities.\n",
        "    - The `V` features have varying distributions, some appearing more Gaussian-like, others skewed.\n",
        "- **Class Imbalance:** Similar to the e-commerce data, the `Class` target variable is extremely imbalanced, with a very small percentage of fraudulent transactions. This is the primary challenge for modeling this dataset and necessitates robust imbalance handling strategies.\n",
        "- **Relationships:** Correlation analysis among `V` features is crucial, as PCA aims to create uncorrelated components. However, their relationship with the `Amount`, `Time`, and `Class` (target) is key. Some `V` features (e.g., V17, V14, V12, V10) are often reported as highly correlated with fraud in similar datasets.\n",
        "- **Temporal Patterns:** While `Time` is not a standard datetime, its numerical nature allows for plotting trends of fraud occurrences over time, which can reveal specific windows of increased fraudulent activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiFaTmZV2wP0"
      },
      "source": [
        "### Overall Recommendations for Modeling\n",
        "\n",
        "- **Imbalance Handling:** Given the severe class imbalance in both datasets, robust techniques like SMOTE, ADASYN, or Borderline-SMOTE should be applied to the training data. Alternatively, using class weights in models (e.g., Logistic Regression, XGBoost, LightGBM) or employing anomaly detection algorithms could be considered.\n",
        "- **Evaluation Metrics:** Accuracy is misleading for imbalanced datasets. Focus on metrics like Precision, Recall, F1-score, AUC-ROC, and Confusion Matrix to evaluate model performance effectively.\n",
        "- **Feature Scaling:** All numerical features (including engineered ones) should be scaled (e.g., `StandardScaler`, `MinMaxScaler`) before training most machine learning models, especially those sensitive to feature scales (e.g., SVMs, Logistic Regression, Neural Networks).\n",
        "- **Categorical Encoding:** One-Hot Encoding has been applied for categorical features. For high-cardinality features not handled by aggregation (e.g., `AccountId` if not aggregated), alternative encoding methods like Target Encoding or Frequency Encoding might be explored if direct OHE leads to too many sparse features.\n",
        "- **Outlier Treatment:** While outliers were visualized, a decision on how to treat them (e.g., winsorization, removal, or using models robust to outliers like tree-based models) should be made based on further experimentation.\n",
        "- **Temporal Aspects:** For the e-commerce data, the engineered temporal features are valuable. For both datasets, consider time-based splitting of data (e.g., training on older data, testing on newer data) to ensure the model generalizes well to future, unseen transactions, as fraud patterns can evolve over time."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}